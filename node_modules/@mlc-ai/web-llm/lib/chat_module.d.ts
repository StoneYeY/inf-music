import { ChatOptions, AppConfig, GenerationConfig } from "./config";
import { ChatCompletion, ChatCompletionChunk, ChatCompletionFinishReason, ChatCompletionMessageParam, ChatCompletionRequestNonStreaming, ChatCompletionRequestStreaming, ChatCompletionRequestBase } from "./openai_api_protocols/index";
import { InitProgressCallback, ChatInterface, GenerateProgressCallback, LogitProcessor } from "./types";
/**
 * This is the main interface to the chat module.
 */
export declare class ChatModule implements ChatInterface {
    private currentModelId?;
    private logger;
    private logitProcessorRegistry?;
    private logitProcessor?;
    private pipeline?;
    private initProgressCallback?;
    private interruptSignal;
    private deviceLostIsError;
    constructor(logitProcessorRegistry?: Map<string, LogitProcessor>);
    setInitProgressCallback(initProgressCallback: InitProgressCallback): void;
    reload(modelId: string, chatOpts?: ChatOptions, appConfig?: AppConfig): Promise<void>;
    generate(input: string | Array<ChatCompletionMessageParam>, progressCallback?: GenerateProgressCallback, streamInterval?: number, genConfig?: GenerationConfig): Promise<string>;
    /**
     * Similar to `generate()`; but instead of using callback, we use an async iterable.
     * @param request Request for chat completion.
     * @param genConfig Generation config extraced from `request`.
     */
    chatCompletionAsyncChunkGenerator(request: ChatCompletionRequestStreaming, genConfig: GenerationConfig): AsyncGenerator<ChatCompletionChunk, void, void>;
    /**
     * Completes a single ChatCompletionRequest.
     *
     * @param request A OpenAI-style ChatCompletion request.
     *
     * @note For each choice (i.e. `n`), a request is defined by a single `prefill()` and mulitple
     * `decode()`. This is important as it determines the behavior of various fields including
     * `stateful` and `seed`.
     */
    chatCompletion(request: ChatCompletionRequestNonStreaming): Promise<ChatCompletion>;
    chatCompletion(request: ChatCompletionRequestStreaming): Promise<AsyncIterable<ChatCompletionChunk>>;
    chatCompletion(request: ChatCompletionRequestBase): Promise<AsyncIterable<ChatCompletionChunk> | ChatCompletion>;
    interruptGenerate(): Promise<void>;
    runtimeStatsText(): Promise<string>;
    resetChat(keepStats?: boolean): Promise<void>;
    unload(): Promise<void>;
    getMaxStorageBufferBindingSize(): Promise<number>;
    getGPUVendor(): Promise<string>;
    forwardTokensAndSample(inputIds: Array<number>, isPrefill: boolean): Promise<number>;
    /**
     * @returns Whether the generation stopped.
     */
    stopped(): boolean;
    /**
     * @returns Finish reason; undefined if generation not started/stopped yet.
    */
    getFinishReason(): ChatCompletionFinishReason | undefined;
    /**
     * Get the current generated response.
     *
     * @returns The current output message.
     */
    getMessage(): Promise<string>;
    /**
     * Modify this.getPipeline().conversation according to the user provided messages.
     * This include modifying `Conversation.messges` and `Conversation.config.system`.
     *
     * @param input The messages from ChatCompletionRequest
     * @note `input[-1]` is not included as it would be treated as a normal input to `prefill()`.
     */
    private updateConversationWithChatCompletionMessages;
    private checkFunctionCallUsage;
    /**
     * Run a prefill step with a given input.
     * @param input The input prompt, or `messages` in OpenAI-like APIs.
     */
    prefill(input: string | Array<ChatCompletionMessageParam>, genConfig?: GenerationConfig): Promise<void>;
    /**
     * Run a decode step to decode the next token.
     */
    decode(genConfig?: GenerationConfig): Promise<void>;
    private getPipeline;
    private asyncLoadTokenizer;
}
/**
 * This is the interface to the chat module that connects to the REST API.
 */
export declare class ChatRestModule implements ChatInterface {
    private logger;
    private initProgressCallback?;
    setInitProgressCallback(initProgressCallback: InitProgressCallback): void;
    reload(modelId: string, chatOpts?: ChatOptions, appConfig?: AppConfig): Promise<void>;
    getMaxStorageBufferBindingSize(): Promise<number>;
    getGPUVendor(): Promise<string>;
    getMessage(): Promise<string>;
    unload(): Promise<void>;
    interruptGenerate(): Promise<void>;
    forwardTokensAndSample(inputIds: Array<number>, isPrefill: boolean): Promise<number>;
    chatCompletion(request: ChatCompletionRequestNonStreaming): Promise<ChatCompletion>;
    chatCompletion(request: ChatCompletionRequestStreaming): Promise<AsyncIterable<ChatCompletionChunk>>;
    chatCompletion(request: ChatCompletionRequestBase): Promise<AsyncIterable<ChatCompletionChunk> | ChatCompletion>;
    generate(input: string | Array<ChatCompletionMessageParam>, progressCallback?: GenerateProgressCallback, streamInterval?: number, genConfig?: GenerationConfig): Promise<string>;
    runtimeStatsText(): Promise<string>;
    resetChat(keepStats?: boolean): Promise<void>;
}
//# sourceMappingURL=chat_module.d.ts.map